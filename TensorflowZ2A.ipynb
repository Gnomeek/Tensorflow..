{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import Latex\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello you god damn tensorflow'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('hello you god damn tensorflow')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_26:0\", shape=(), dtype=float32) node2: Tensor(\"Const_27:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add_24:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)\n",
    "print('node1:', node1, 'node2:', node2)\n",
    "print('node3:', node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1,node2): [3.0, 4.0]\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print('sess.run(node1,node2):', sess.run([node1, node2]))\n",
    "print('sess.run(node3):', sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.299999\n",
      "[6. 9.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a+b\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3.6, b: 10.7}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1, 3], b: [5, 6]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 4.0], [[4, 6]]]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3\n",
    "[1, 3, 4]\n",
    "[[1, 45, 617], [51, 2512, 71]]\n",
    "[[[1., 4.], [[4, 6]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_model = tf.mul(X, w)  # linear function\\ncost = tf.square(Y-y_model)  # cost fuction\\ntrain_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  \\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample code\n",
    "'''\n",
    "y_model = tf.mul(X, w)  # linear function\n",
    "cost = tf.square(Y-y_model)  # cost fuction\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)  \n",
    "'''\n",
    "# try to get the min cost of the convex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0892153 [-0.2480714] [2.7214437]\n",
      "20 0.9784155 [-0.1528319] [2.6096394]\n",
      "40 0.88851434 [-0.09516941] [2.4885283]\n",
      "60 0.8069621 [-0.04336881] [2.3717227]\n",
      "80 0.7328958 [0.00569729] [2.2602742]\n",
      "100 0.6656277 [0.0524289] [2.1540508]\n",
      "120 0.6045337 [0.0969616] [2.0528185]\n",
      "140 0.54904735 [0.139401] [1.9563438]\n",
      "160 0.49865362 [0.17984602] [1.8644028]\n",
      "180 0.45288503 [0.21839024] [1.7767825]\n",
      "200 0.41131738 [0.25512308] [1.6932803]\n",
      "220 0.3735651 [0.29012954] [1.6137024]\n",
      "240 0.33927783 [0.32349083] [1.5378643]\n",
      "260 0.3081376 [0.35528424] [1.4655905]\n",
      "280 0.2798555 [0.38558352] [1.396713]\n",
      "300 0.25416914 [0.4144589] [1.3310726]\n",
      "320 0.23084043 [0.44197717] [1.2685169]\n",
      "340 0.20965306 [0.46820214] [1.2089015]\n",
      "360 0.19041021 [0.49319473] [1.1520875]\n",
      "380 0.1729336 [0.5170128] [1.0979434]\n",
      "400 0.15706113 [0.5397114] [1.0463443]\n",
      "420 0.14264536 [0.56134325] [0.9971699]\n",
      "440 0.12955283 [0.5819585] [0.9503066]\n",
      "460 0.11766198 [0.6016049] [0.90564567]\n",
      "480 0.10686246 [0.62032807] [0.86308366]\n",
      "500 0.09705422 [0.63817126] [0.822522]\n",
      "520 0.08814622 [0.65517586] [0.7838664]\n",
      "540 0.0800558 [0.6713813] [0.7470277]\n",
      "560 0.07270795 [0.6868251] [0.7119202]\n",
      "580 0.06603455 [0.70154315] [0.67846256]\n",
      "600 0.05997364 [0.7155695] [0.6465774]\n",
      "620 0.054469 [0.72893673] [0.6161906]\n",
      "640 0.04946967 [0.74167573] [0.58723193]\n",
      "660 0.044929132 [0.753816] [0.5596342]\n",
      "680 0.040805344 [0.76538575] [0.5333334]\n",
      "700 0.03706008 [0.7764118] [0.5082687]\n",
      "720 0.033658534 [0.7869196] [0.48438185]\n",
      "740 0.030569227 [0.7969336] [0.4616177]\n",
      "760 0.027763462 [0.806477] [0.43992338]\n",
      "780 0.025215214 [0.8155718] [0.41924858]\n",
      "800 0.022900857 [0.82423925] [0.39954543]\n",
      "820 0.020798914 [0.8324994] [0.38076818]\n",
      "840 0.018889917 [0.8403714] [0.36287344]\n",
      "860 0.017156104 [0.84787333] [0.34581974]\n",
      "880 0.015581456 [0.85502267] [0.32956746]\n",
      "900 0.0141513245 [0.8618361] [0.31407902]\n",
      "920 0.012852461 [0.86832935] [0.29931855]\n",
      "940 0.011672824 [0.8745173] [0.28525165]\n",
      "960 0.010601425 [0.88041455] [0.27184582]\n",
      "980 0.009628395 [0.8860346] [0.25907013]\n",
      "1000 0.008744669 [0.89139056] [0.24689484]\n",
      "1020 0.007942057 [0.8964948] [0.23529172]\n",
      "1040 0.007213092 [0.90135914] [0.22423387]\n",
      "1060 0.0065510445 [0.9059949] [0.21369569]\n",
      "1080 0.005949773 [0.9104128] [0.20365281]\n",
      "1100 0.0054036756 [0.914623] [0.19408192]\n",
      "1120 0.004907712 [0.91863537] [0.18496083]\n",
      "1140 0.0044572563 [0.92245924] [0.17626832]\n",
      "1160 0.0040481524 [0.9261034] [0.1679844]\n",
      "1180 0.0036765954 [0.92957634] [0.16008972]\n",
      "1200 0.003339141 [0.9328859] [0.15256608]\n",
      "1220 0.0030326678 [0.93604] [0.14539608]\n",
      "1240 0.0027543188 [0.9390457] [0.13856307]\n",
      "1260 0.002501515 [0.9419105] [0.13205115]\n",
      "1280 0.0022719207 [0.9446405] [0.1258452]\n",
      "1300 0.0020633913 [0.9472422] [0.11993092]\n",
      "1320 0.0018740053 [0.9497216] [0.11429466]\n",
      "1340 0.0017020075 [0.9520845] [0.10892323]\n",
      "1360 0.0015457874 [0.9543364] [0.10380421]\n",
      "1380 0.0014039036 [0.9564824] [0.09892575]\n",
      "1400 0.0012750467 [0.9585276] [0.09427658]\n",
      "1420 0.001158015 [0.9604767] [0.08984589]\n",
      "1440 0.0010517308 [0.9623341] [0.08562343]\n",
      "1460 0.0009551972 [0.9641043] [0.08159942]\n",
      "1480 0.0008675259 [0.9657912] [0.07776454]\n",
      "1500 0.0007879045 [0.9673989] [0.07410991]\n",
      "1520 0.0007155879 [0.96893096] [0.07062706]\n",
      "1540 0.0006499064 [0.9703912] [0.06730787]\n",
      "1560 0.0005902559 [0.97178274] [0.06414462]\n",
      "1580 0.0005360792 [0.97310877] [0.06113005]\n",
      "1600 0.0004868745 [0.9743726] [0.05825716]\n",
      "1620 0.0004421849 [0.975577] [0.05551923]\n",
      "1640 0.00040160122 [0.9767248] [0.05291001]\n",
      "1660 0.00036474128 [0.9778186] [0.05042345]\n",
      "1680 0.0003312636 [0.9788611] [0.04805376]\n",
      "1700 0.000300861 [0.97985446] [0.04579546]\n",
      "1720 0.0002732453 [0.98080134] [0.0436432]\n",
      "1740 0.00024816362 [0.9817036] [0.04159212]\n",
      "1760 0.00022538852 [0.98256344] [0.03963743]\n",
      "1780 0.00020470198 [0.9833829] [0.03777461]\n",
      "1800 0.00018591281 [0.9841638] [0.03599934]\n",
      "1820 0.00016884836 [0.9849081] [0.03430749]\n",
      "1840 0.00015335155 [0.98561734] [0.03269517]\n",
      "1860 0.00013927581 [0.98629326] [0.03115862]\n",
      "1880 0.00012649181 [0.98693734] [0.02969431]\n",
      "1900 0.00011488214 [0.9875513] [0.0282988]\n",
      "1920 0.00010433792 [0.98813635] [0.02696888]\n",
      "1940 9.47609e-05 [0.98869383] [0.02570145]\n",
      "1960 8.6064596e-05 [0.98922527] [0.02449359]\n",
      "1980 7.816471e-05 [0.98973143] [0.02334255]\n",
      "2000 7.0990856e-05 [0.99021417] [0.02224563]\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# cost function\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = x_train*W+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y_train))\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "        \n",
    "# learn rate cause W swap between 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.0990856e-05 [0.9902377] [0.02219215]\n",
      "20 6.447496e-05 [0.9906965] [0.02114915]\n",
      "40 5.8557187e-05 [0.99113363] [0.02015521]\n",
      "60 5.3183187e-05 [0.9915504] [0.01920799]\n",
      "80 4.830152e-05 [0.9919475] [0.01830528]\n",
      "100 4.3868455e-05 [0.9923259] [0.017445]\n",
      "120 3.9842376e-05 [0.99268657] [0.01662516]\n",
      "140 3.618553e-05 [0.99303025] [0.01584384]\n",
      "160 3.2863925e-05 [0.9933578] [0.01509927]\n",
      "180 2.9847646e-05 [0.99367] [0.01438967]\n",
      "200 2.710807e-05 [0.9939675] [0.01371341]\n",
      "220 2.4620545e-05 [0.99425095] [0.0130689]\n",
      "240 2.2360213e-05 [0.99452114] [0.01245472]\n",
      "260 2.0307587e-05 [0.99477863] [0.01186941]\n",
      "280 1.8443834e-05 [0.9950241] [0.01131157]\n",
      "300 1.675108e-05 [0.9952579] [0.01077994]\n",
      "320 1.5213945e-05 [0.9954807] [0.01027334]\n",
      "340 1.3817514e-05 [0.9956931] [0.00979053]\n",
      "360 1.254869e-05 [0.99589556] [0.00933041]\n",
      "380 1.1397276e-05 [0.99608845] [0.00889192]\n",
      "400 1.0351421e-05 [0.99627227] [0.00847403]\n",
      "420 9.401382e-06 [0.99644744] [0.00807579]\n",
      "440 8.538459e-06 [0.99661446] [0.00769624]\n",
      "460 7.754649e-06 [0.99677354] [0.00733451]\n",
      "480 7.042827e-06 [0.9969252] [0.00698981]\n",
      "500 6.396218e-06 [0.99706966] [0.00666131]\n",
      "520 5.809225e-06 [0.9972074] [0.00634825]\n",
      "540 5.2760483e-06 [0.99733865] [0.0060499]\n",
      "560 4.7916524e-06 [0.9974637] [0.00576556]\n",
      "580 4.3518735e-06 [0.99758285] [0.00549464]\n",
      "600 3.952558e-06 [0.99769646] [0.00523642]\n",
      "620 3.5897663e-06 [0.99780476] [0.00499034]\n",
      "640 3.2604185e-06 [0.9979079] [0.00475584]\n",
      "660 2.960964e-06 [0.9980062] [0.00453235]\n",
      "680 2.6894857e-06 [0.99809986] [0.00431937]\n",
      "700 2.4424562e-06 [0.99818915] [0.0041164]\n",
      "720 2.2182858e-06 [0.99827427] [0.00392296]\n",
      "740 2.0147081e-06 [0.9983553] [0.00373861]\n",
      "760 1.8299528e-06 [0.99843264] [0.00356295]\n",
      "780 1.6619266e-06 [0.9985063] [0.00339552]\n",
      "800 1.5094262e-06 [0.9985765] [0.00323594]\n",
      "820 1.3709368e-06 [0.99864334] [0.00308388]\n",
      "840 1.2450217e-06 [0.9987071] [0.00293898]\n",
      "860 1.1309112e-06 [0.99876785] [0.00280089]\n",
      "880 1.0269742e-06 [0.9988258] [0.00266927]\n",
      "900 9.3287093e-07 [0.9988809] [0.00254383]\n",
      "920 8.472395e-07 [0.9989335] [0.00242431]\n",
      "940 7.6943564e-07 [0.9989836] [0.00231041]\n",
      "960 6.9895214e-07 [0.99903136] [0.00220184]\n",
      "980 6.3476597e-07 [0.99907684] [0.00209839]\n",
      "1000 5.7649646e-07 [0.99912024] [0.00199984]\n",
      "1020 5.236886e-07 [0.99916154] [0.00190588]\n",
      "1040 4.7547496e-07 [0.99920094] [0.00181632]\n",
      "1060 4.3186938e-07 [0.9992385] [0.00173099]\n",
      "1080 3.9227174e-07 [0.99927425] [0.00164968]\n",
      "1100 3.56299e-07 [0.99930835] [0.00157218]\n",
      "1120 3.2362013e-07 [0.9993409] [0.00149832]\n",
      "1140 2.9393064e-07 [0.99937177] [0.00142795]\n",
      "1160 2.669687e-07 [0.9994012] [0.00136091]\n",
      "1180 2.4255021e-07 [0.99942935] [0.00129701]\n",
      "1200 2.2028634e-07 [0.9994561] [0.00123611]\n",
      "1220 2.0011396e-07 [0.9994817] [0.00117806]\n",
      "1240 1.817392e-07 [0.999506] [0.00112274]\n",
      "1260 1.650392e-07 [0.9995292] [0.00107002]\n",
      "1280 1.4992281e-07 [0.9995513] [0.00101977]\n",
      "1300 1.3616959e-07 [0.9995724] [0.00097188]\n",
      "1320 1.2369111e-07 [0.99959254] [0.00092625]\n",
      "1340 1.12323484e-07 [0.9996116] [0.00088279]\n",
      "1360 1.02049945e-07 [0.99962974] [0.00084137]\n",
      "1380 9.269269e-08 [0.99964714] [0.00080188]\n",
      "1400 8.419884e-08 [0.99966383] [0.00076428]\n",
      "1420 7.6479296e-08 [0.99967945] [0.00072842]\n",
      "1440 6.947111e-08 [0.9996945] [0.00069422]\n",
      "1460 6.3149265e-08 [0.99970883] [0.00066171]\n",
      "1480 5.7329718e-08 [0.9997225] [0.00063065]\n",
      "1500 5.208601e-08 [0.9997356] [0.00060108]\n",
      "1520 4.7319876e-08 [0.9997479] [0.00057285]\n",
      "1540 4.2963055e-08 [0.9997598] [0.00054602]\n",
      "1560 3.9045176e-08 [0.99977094] [0.00052037]\n",
      "1580 3.546613e-08 [0.9997817] [0.00049602]\n",
      "1600 3.2218583e-08 [0.99979204] [0.00047271]\n",
      "1620 2.9281944e-08 [0.99980164] [0.00045058]\n",
      "1640 2.6573014e-08 [0.9998111] [0.00042946]\n",
      "1660 2.4146502e-08 [0.9998199] [0.00040928]\n",
      "1680 2.1958025e-08 [0.9998282] [0.00039016]\n",
      "1700 1.9927805e-08 [0.9998365] [0.00037186]\n",
      "1720 1.8113349e-08 [0.999844] [0.00035439]\n",
      "1740 1.6456587e-08 [0.99985117] [0.00033788]\n",
      "1760 1.4957024e-08 [0.9998583] [0.00032209]\n",
      "1780 1.3578003e-08 [0.999865] [0.00030693]\n",
      "1800 1.2347646e-08 [0.99987113] [0.00029257]\n",
      "1820 1.1230767e-08 [0.99987715] [0.00027894]\n",
      "1840 1.018375e-08 [0.9998831] [0.00026587]\n",
      "1860 9.255572e-09 [0.9998886] [0.00025334]\n",
      "1880 8.404129e-09 [0.99989367] [0.00024146]\n",
      "1900 7.6363635e-09 [0.9998985] [0.00023023]\n",
      "1920 6.9553843e-09 [0.99990326] [0.00021956]\n",
      "1940 6.303651e-09 [0.99990803] [0.00020927]\n",
      "1960 5.7308633e-09 [0.9999123] [0.00019939]\n",
      "1980 5.1987095e-09 [0.9999163] [0.00019004]\n",
      "2000 4.7293915e-09 [0.9999201] [0.00018118]\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "X=tf.placeholder(tf.float32)\n",
    "Y=tf.placeholder(tf.float32)\n",
    "for step in range(2001):\n",
    "    cost_val,W_val,b_val,_=\\\n",
    "    sess.run([cost,W,b,train],\n",
    "             feed_dict={X:[1,2,3],Y:[1,2,3]})\n",
    "    if step % 20==0:\n",
    "        print(step,cost_val,W_val,b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$H(x) = \\frac{1}{1+e^-XW}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid function looks like 'S' shape\n",
    "Latex(r\"$H(x) = \\frac{1}{1+e^-XW}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.9463236\n",
      "20 1.7381445\n",
      "40 1.1139535\n",
      "60 0.9436195\n",
      "80 0.88794726\n",
      "100 0.8515503\n",
      "120 0.8201676\n",
      "140 0.79167\n",
      "160 0.76561886\n",
      "180 0.7418106\n",
      "200 0.7200713\n",
      "220 0.7002321\n",
      "240 0.68212813\n",
      "260 0.6656025\n",
      "280 0.65050656\n",
      "300 0.63670224\n",
      "320 0.62406224\n",
      "340 0.61246985\n",
      "360 0.6018193\n",
      "380 0.5920151\n",
      "400 0.5829709\n",
      "420 0.5746098\n",
      "440 0.5668623\n",
      "460 0.55966675\n",
      "480 0.55296797\n",
      "500 0.5467168\n",
      "520 0.54086924\n",
      "540 0.53538615\n",
      "560 0.53023267\n",
      "580 0.5253775\n",
      "600 0.5207929\n",
      "620 0.5164538\n",
      "640 0.5123379\n",
      "660 0.5084255\n",
      "680 0.5046982\n",
      "700 0.5011402\n",
      "720 0.49773702\n",
      "740 0.49447563\n",
      "760 0.49134418\n",
      "780 0.48833203\n",
      "800 0.48542988\n",
      "820 0.48262882\n",
      "840 0.4799211\n",
      "860 0.47729945\n",
      "880 0.47475758\n",
      "900 0.47228968\n",
      "920 0.46989015\n",
      "940 0.46755433\n",
      "960 0.4652777\n",
      "980 0.4630562\n",
      "1000 0.46088603\n",
      "1020 0.458764\n",
      "1040 0.45668688\n",
      "1060 0.4546518\n",
      "1080 0.45265618\n",
      "1100 0.45069763\n",
      "1120 0.44877395\n",
      "1140 0.446883\n",
      "1160 0.44502306\n",
      "1180 0.44319233\n",
      "1200 0.44138917\n",
      "1220 0.43961203\n",
      "1240 0.43785977\n",
      "1260 0.43613112\n",
      "1280 0.43442473\n",
      "1300 0.4327396\n",
      "1320 0.43107477\n",
      "1340 0.42942926\n",
      "1360 0.42780232\n",
      "1380 0.426193\n",
      "1400 0.42460057\n",
      "1420 0.4230245\n",
      "1440 0.42146388\n",
      "1460 0.41991833\n",
      "1480 0.4183872\n",
      "1500 0.41687\n",
      "1520 0.4153662\n",
      "1540 0.4138753\n",
      "1560 0.41239703\n",
      "1580 0.41093087\n",
      "1600 0.40947637\n",
      "1620 0.40803337\n",
      "1640 0.40660146\n",
      "1660 0.4051803\n",
      "1680 0.40376958\n",
      "1700 0.4023691\n",
      "1720 0.4009786\n",
      "1740 0.3995978\n",
      "1760 0.3982264\n",
      "1780 0.39686438\n",
      "1800 0.3955114\n",
      "1820 0.3941672\n",
      "1840 0.39283177\n",
      "1860 0.39150488\n",
      "1880 0.39018628\n",
      "1900 0.38887596\n",
      "1920 0.38757372\n",
      "1940 0.38627934\n",
      "1960 0.38499287\n",
      "1980 0.38371396\n",
      "2000 0.38244247\n",
      "2020 0.38117862\n",
      "2040 0.379922\n",
      "2060 0.37867275\n",
      "2080 0.37743056\n",
      "2100 0.37619546\n",
      "2120 0.37496734\n",
      "2140 0.37374607\n",
      "2160 0.37253153\n",
      "2180 0.3713238\n",
      "2200 0.37012264\n",
      "2220 0.36892807\n",
      "2240 0.36774004\n",
      "2260 0.36655834\n",
      "2280 0.36538306\n",
      "2300 0.36421415\n",
      "2320 0.36305133\n",
      "2340 0.36189476\n",
      "2360 0.36074445\n",
      "2380 0.3596001\n",
      "2400 0.35846174\n",
      "2420 0.35732937\n",
      "2440 0.35620293\n",
      "2460 0.35508236\n",
      "2480 0.35396767\n",
      "2500 0.35285866\n",
      "2520 0.35175547\n",
      "2540 0.35065794\n",
      "2560 0.34956607\n",
      "2580 0.34847984\n",
      "2600 0.3473991\n",
      "2620 0.34632394\n",
      "2640 0.34525418\n",
      "2660 0.34419\n",
      "2680 0.3431311\n",
      "2700 0.3420777\n",
      "2720 0.34102955\n",
      "2740 0.3399867\n",
      "2760 0.3389492\n",
      "2780 0.33791688\n",
      "2800 0.33688974\n",
      "2820 0.3358678\n",
      "2840 0.33485094\n",
      "2860 0.3338393\n",
      "2880 0.33283255\n",
      "2900 0.33183095\n",
      "2920 0.33083436\n",
      "2940 0.32984275\n",
      "2960 0.32885602\n",
      "2980 0.32787427\n",
      "3000 0.32689735\n",
      "3020 0.32592535\n",
      "3040 0.32495812\n",
      "3060 0.32399568\n",
      "3080 0.32303807\n",
      "3100 0.32208517\n",
      "3120 0.32113695\n",
      "3140 0.32019347\n",
      "3160 0.31925455\n",
      "3180 0.3183204\n",
      "3200 0.3173907\n",
      "3220 0.31646565\n",
      "3240 0.31554517\n",
      "3260 0.31462917\n",
      "3280 0.31371763\n",
      "3300 0.31281063\n",
      "3320 0.31190807\n",
      "3340 0.31100985\n",
      "3360 0.31011608\n",
      "3380 0.3092267\n",
      "3400 0.3083416\n",
      "3420 0.30746076\n",
      "3440 0.30658433\n",
      "3460 0.30571213\n",
      "3480 0.3048441\n",
      "3500 0.3039804\n",
      "3520 0.30312082\n",
      "3540 0.3022654\n",
      "3560 0.30141413\n",
      "3580 0.30056703\n",
      "3600 0.29972398\n",
      "3620 0.29888502\n",
      "3640 0.29805005\n",
      "3660 0.29721913\n",
      "3680 0.2963923\n",
      "3700 0.29556933\n",
      "3720 0.2947503\n",
      "3740 0.2939353\n",
      "3760 0.29312417\n",
      "3780 0.29231694\n",
      "3800 0.29151353\n",
      "3820 0.29071394\n",
      "3840 0.28991827\n",
      "3860 0.28912628\n",
      "3880 0.28833815\n",
      "3900 0.28755373\n",
      "3920 0.2867731\n",
      "3940 0.2859961\n",
      "3960 0.28522286\n",
      "3980 0.28445324\n",
      "4000 0.28368723\n",
      "4020 0.2829249\n",
      "4040 0.2821662\n",
      "4060 0.28141102\n",
      "4080 0.28065944\n",
      "4100 0.27991143\n",
      "4120 0.27916688\n",
      "4140 0.27842584\n",
      "4160 0.2776883\n",
      "4180 0.2769542\n",
      "4200 0.27622357\n",
      "4220 0.27549633\n",
      "4240 0.27477255\n",
      "4260 0.27405205\n",
      "4280 0.27333498\n",
      "4300 0.2726212\n",
      "4320 0.27191082\n",
      "4340 0.2712037\n",
      "4360 0.27049983\n",
      "4380 0.26979923\n",
      "4400 0.26910192\n",
      "4420 0.26840782\n",
      "4440 0.2677169\n",
      "4460 0.2670292\n",
      "4480 0.26634467\n",
      "4500 0.26566327\n",
      "4520 0.264985\n",
      "4540 0.2643099\n",
      "4560 0.2636378\n",
      "4580 0.26296887\n",
      "4600 0.262303\n",
      "4620 0.2616401\n",
      "4640 0.26098028\n",
      "4660 0.2603235\n",
      "4680 0.25966963\n",
      "4700 0.25901875\n",
      "4720 0.2583709\n",
      "4740 0.25772595\n",
      "4760 0.25708392\n",
      "4780 0.25644478\n",
      "4800 0.25580847\n",
      "4820 0.25517514\n",
      "4840 0.25454462\n",
      "4860 0.25391695\n",
      "4880 0.2532921\n",
      "4900 0.25267005\n",
      "4920 0.25205073\n",
      "4940 0.25143424\n",
      "4960 0.25082043\n",
      "4980 0.25020948\n",
      "5000 0.24960123\n",
      "5020 0.24899568\n",
      "5040 0.24839284\n",
      "5060 0.2477926\n",
      "5080 0.24719507\n",
      "5100 0.2466002\n",
      "5120 0.24600796\n",
      "5140 0.24541835\n",
      "5160 0.24483132\n",
      "5180 0.24424694\n",
      "5200 0.24366504\n",
      "5220 0.24308573\n",
      "5240 0.24250896\n",
      "5260 0.24193476\n",
      "5280 0.24136306\n",
      "5300 0.24079384\n",
      "5320 0.24022709\n",
      "5340 0.23966277\n",
      "5360 0.23910104\n",
      "5380 0.23854168\n",
      "5400 0.2379847\n",
      "5420 0.23743017\n",
      "5440 0.23687805\n",
      "5460 0.23632832\n",
      "5480 0.23578097\n",
      "5500 0.23523597\n",
      "5520 0.23469333\n",
      "5540 0.23415297\n",
      "5560 0.23361498\n",
      "5580 0.23307931\n",
      "5600 0.23254591\n",
      "5620 0.23201476\n",
      "5640 0.23148589\n",
      "5660 0.23095931\n",
      "5680 0.23043495\n",
      "5700 0.22991282\n",
      "5720 0.22939289\n",
      "5740 0.22887518\n",
      "5760 0.22835968\n",
      "5780 0.22784632\n",
      "5800 0.22733517\n",
      "5820 0.22682619\n",
      "5840 0.22631931\n",
      "5860 0.22581452\n",
      "5880 0.22531192\n",
      "5900 0.22481133\n",
      "5920 0.22431298\n",
      "5940 0.22381657\n",
      "5960 0.22332227\n",
      "5980 0.22283007\n",
      "6000 0.22233994\n",
      "6020 0.22185177\n",
      "6040 0.22136568\n",
      "6060 0.22088154\n",
      "6080 0.22039948\n",
      "6100 0.21991938\n",
      "6120 0.21944125\n",
      "6140 0.2189651\n",
      "6160 0.21849082\n",
      "6180 0.21801859\n",
      "6200 0.2175483\n",
      "6220 0.21707988\n",
      "6240 0.21661349\n",
      "6260 0.21614887\n",
      "6280 0.21568614\n",
      "6300 0.21522535\n",
      "6320 0.21476646\n",
      "6340 0.21430933\n",
      "6360 0.21385415\n",
      "6380 0.21340078\n",
      "6400 0.21294923\n",
      "6420 0.21249948\n",
      "6440 0.21205159\n",
      "6460 0.21160544\n",
      "6480 0.21116115\n",
      "6500 0.21071859\n",
      "6520 0.21027781\n",
      "6540 0.20983882\n",
      "6560 0.20940155\n",
      "6580 0.20896608\n",
      "6600 0.20853226\n",
      "6620 0.2081002\n",
      "6640 0.20766987\n",
      "6660 0.20724128\n",
      "6680 0.20681429\n",
      "6700 0.20638907\n",
      "6720 0.20596547\n",
      "6740 0.20554358\n",
      "6760 0.2051233\n",
      "6780 0.20470472\n",
      "6800 0.20428783\n",
      "6820 0.20387252\n",
      "6840 0.20345877\n",
      "6860 0.20304666\n",
      "6880 0.20263618\n",
      "6900 0.20222725\n",
      "6920 0.20181997\n",
      "6940 0.20141427\n",
      "6960 0.20101012\n",
      "6980 0.20060754\n",
      "7000 0.20020653\n",
      "7020 0.19980705\n",
      "7040 0.19940908\n",
      "7060 0.19901268\n",
      "7080 0.1986178\n",
      "7100 0.19822443\n",
      "7120 0.19783257\n",
      "7140 0.19744217\n",
      "7160 0.19705331\n",
      "7180 0.19666587\n",
      "7200 0.19627996\n",
      "7220 0.1958955\n",
      "7240 0.19551247\n",
      "7260 0.19513094\n",
      "7280 0.19475083\n",
      "7300 0.1943721\n",
      "7320 0.19399484\n",
      "7340 0.19361901\n",
      "7360 0.19324459\n",
      "7380 0.19287159\n",
      "7400 0.1925\n",
      "7420 0.19212975\n",
      "7440 0.19176097\n",
      "7460 0.19139345\n",
      "7480 0.19102736\n",
      "7500 0.19066262\n",
      "7520 0.19029929\n",
      "7540 0.18993728\n",
      "7560 0.18957657\n",
      "7580 0.18921722\n",
      "7600 0.18885921\n",
      "7620 0.18850248\n",
      "7640 0.18814713\n",
      "7660 0.187793\n",
      "7680 0.18744026\n",
      "7700 0.18708874\n",
      "7720 0.1867385\n",
      "7740 0.18638958\n",
      "7760 0.1860419\n",
      "7780 0.18569553\n",
      "7800 0.1853504\n",
      "7820 0.18500651\n",
      "7840 0.18466388\n",
      "7860 0.18432252\n",
      "7880 0.18398231\n",
      "7900 0.18364342\n",
      "7920 0.18330573\n",
      "7940 0.18296921\n",
      "7960 0.18263392\n",
      "7980 0.18229987\n",
      "8000 0.18196698\n",
      "8020 0.18163526\n",
      "8040 0.18130474\n",
      "8060 0.18097548\n",
      "8080 0.18064733\n",
      "8100 0.18032037\n",
      "8120 0.17999452\n",
      "8140 0.17966984\n",
      "8160 0.17934637\n",
      "8180 0.17902397\n",
      "8200 0.17870277\n",
      "8220 0.17838264\n",
      "8240 0.17806368\n",
      "8260 0.17774582\n",
      "8280 0.1774291\n",
      "8300 0.17711349\n",
      "8320 0.17679892\n",
      "8340 0.17648552\n",
      "8360 0.17617321\n",
      "8380 0.17586195\n",
      "8400 0.17555177\n",
      "8420 0.1752427\n",
      "8440 0.1749347\n",
      "8460 0.17462778\n",
      "8480 0.17432187\n",
      "8500 0.17401706\n",
      "8520 0.17371327\n",
      "8540 0.17341058\n",
      "8560 0.17310888\n",
      "8580 0.17280819\n",
      "8600 0.17250861\n",
      "8620 0.17220998\n",
      "8640 0.17191242\n",
      "8660 0.1716159\n",
      "8680 0.17132032\n",
      "8700 0.17102575\n",
      "8720 0.17073226\n",
      "8740 0.17043974\n",
      "8760 0.17014813\n",
      "8780 0.16985756\n",
      "8800 0.16956799\n",
      "8820 0.16927934\n",
      "8840 0.16899179\n",
      "8860 0.16870509\n",
      "8880 0.16841938\n",
      "8900 0.16813459\n",
      "8920 0.1678508\n",
      "8940 0.16756801\n",
      "8960 0.16728608\n",
      "8980 0.16700512\n",
      "9000 0.1667251\n",
      "9020 0.16644599\n",
      "9040 0.16616781\n",
      "9060 0.16589057\n",
      "9080 0.16561428\n",
      "9100 0.16533886\n",
      "9120 0.16506435\n",
      "9140 0.16479073\n",
      "9160 0.16451804\n",
      "9180 0.1642462\n",
      "9200 0.16397531\n",
      "9220 0.16370527\n",
      "9240 0.16343611\n",
      "9260 0.16316788\n",
      "9280 0.16290046\n",
      "9300 0.16263394\n",
      "9320 0.16236831\n",
      "9340 0.16210353\n",
      "9360 0.16183962\n",
      "9380 0.16157657\n",
      "9400 0.1613143\n",
      "9420 0.1610529\n",
      "9440 0.16079243\n",
      "9460 0.16053273\n",
      "9480 0.16027386\n",
      "9500 0.16001584\n",
      "9520 0.15975861\n",
      "9540 0.15950227\n",
      "9560 0.1592467\n",
      "9580 0.15899193\n",
      "9600 0.15873802\n",
      "9620 0.15848492\n",
      "9640 0.15823255\n",
      "9660 0.15798107\n",
      "9680 0.15773037\n",
      "9700 0.15748043\n",
      "9720 0.15723133\n",
      "9740 0.15698297\n",
      "9760 0.1567354\n",
      "9780 0.15648863\n",
      "9800 0.15624258\n",
      "9820 0.15599741\n",
      "9840 0.15575291\n",
      "9860 0.15550923\n",
      "9880 0.15526631\n",
      "9900 0.15502414\n",
      "9920 0.15478265\n",
      "9940 0.15454206\n",
      "9960 0.1543021\n",
      "9980 0.15406288\n",
      "10000 0.15382446\n",
      "\n",
      "hypothesis: [[0.03258222]\n",
      " [0.16127083]\n",
      " [0.3135618 ]\n",
      " [0.7774384 ]\n",
      " [0.9370622 ]\n",
      " [0.9793292 ]] \n",
      "Correct: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuarcy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=[None,2])\n",
    "Y=tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([2,1]),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted=tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val,_=sess.run([cost,train],feed_dict={X:x_data,Y:y_data})\n",
    "        if step % 20==0:\n",
    "            print(step,cost_val)\n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],\n",
    "                   feed_dict={X:x_data,Y:y_data})\n",
    "    print('\\nhypothesis:',h,'\\nCorrect:',c,'\\nAccuarcy:',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*softmax function isn't 'hard' as its name.now you have choose max one in A,B(A>B)*<br>\n",
    "*if using 'hard' function, it will always be A.*<br>\n",
    "**HOWEVER**, *in softmax function, sometimes you may take B as well,relevant with its probaility*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0962545\n",
      "200 0.58344424\n",
      "400 0.48197332\n",
      "600 0.3887133\n",
      "800 0.29759678\n",
      "1000 0.23651946\n",
      "1200 0.21431586\n",
      "1400 0.19581911\n",
      "1600 0.18016206\n",
      "1800 0.16674337\n",
      "2000 0.15512192\n",
      "[[8.6660925e-03 9.9132466e-01 9.1623915e-06]\n",
      " [8.1411171e-01 1.6752347e-01 1.8364852e-02]\n",
      " [1.1199444e-08 3.3190742e-04 9.9966812e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5],                                                        [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "hypothesis=tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost=tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer,feed_dict={X:x_data,Y:y_data})\n",
    "        if step%200==0:\n",
    "            print(step,sess.run(cost,feed_dict={X:x_data,Y:y_data}))\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], \n",
    "                                          [1, 3, 4, 3], \n",
    "                                          [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict number with pic(using google's sample)// unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.563860096\n",
      "Epoch: 0002, Cost: 1.079840734\n",
      "Epoch: 0003, Cost: 0.883682346\n",
      "Epoch: 0004, Cost: 0.782288874\n",
      "Epoch: 0005, Cost: 0.716026228\n",
      "Epoch: 0006, Cost: 0.667871558\n",
      "Epoch: 0007, Cost: 0.631024850\n",
      "Epoch: 0008, Cost: 0.600807719\n",
      "Epoch: 0009, Cost: 0.575272534\n",
      "Epoch: 0010, Cost: 0.554038144\n",
      "Epoch: 0011, Cost: 0.535320992\n",
      "Epoch: 0012, Cost: 0.519478798\n",
      "Epoch: 0013, Cost: 0.504803188\n",
      "Epoch: 0014, Cost: 0.492529576\n",
      "Epoch: 0015, Cost: 0.480649119\n",
      "Learning finished\n",
      "Accuracy:  0.8855\n",
      "Label:  [0]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001, Cost: 2.826302672\\nEpoch: 0002, Cost: 1.061668952\\nEpoch: 0003, Cost: 0.838061315\\nEpoch: 0004, Cost: 0.733232745\\nEpoch: 0005, Cost: 0.669279885\\nEpoch: 0006, Cost: 0.624611836\\nEpoch: 0007, Cost: 0.591160344\\nEpoch: 0008, Cost: 0.563868987\\nEpoch: 0009, Cost: 0.541745171\\nEpoch: 0010, Cost: 0.522673578\\nEpoch: 0011, Cost: 0.506782325\\nEpoch: 0012, Cost: 0.492447643\\nEpoch: 0013, Cost: 0.479955837\\nEpoch: 0014, Cost: 0.468893674\\nEpoch: 0015, Cost: 0.458703488\\nLearning finished\\nAccuracy:  0.8951\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random\n",
    "\n",
    "mnist = input_data.read_data_sets('mnist_data/', one_hot=True)\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs=15\n",
    "batch_size=100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #TRAINING CYCLE\n",
    "    for epochs in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "            c,_=sess.run([cost,optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost+=c/num_iterations\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf36]",
   "language": "python",
   "name": "conda-env-tf36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
